{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query Rewriting\n",
    "\n",
    "In RAG we can often encounter issues with the users query. This could be inaccuracy, ambiguity, or even a lack of information. In general its pretty common that the query is not optimal for the retrieval phase of RAG.\n",
    "\n",
    "Imagine the following scenario. You're building an application that automatically answers support tickets. Queries in this case will be emails from customers. You can imagine the following issues that can arise:\n",
    "\n",
    "- Emails often contain noise, such as greetings, signatures, etc. which are not relevant for the question and can confuse the model.\n",
    "- The user might have multiple questions in the same email, which makes it harder to retrieve the correct documents. \n",
    "- The question posed by the user might be ambiguous, or not specific enough.\n",
    "\n",
    "A group of techniques in the **Pre-retrieval** phase categorised as **Query Rewriting** are trying to remedy issues like these. This notebook will explore these methods of rewriting queries to make them more suitable for the retrieval phase of RAG.\n",
    "\n",
    "The following exmaples will be covered:\n",
    "\n",
    "- **Rewrite-Retrieve-Read**: Explore a technique proposed in the paper [Query Rewriting for Retrieval-Augmented Large Language Models](https://arxiv.org/pdf/2305.14283.pdf)\n",
    "- **Hypothetical Document Embeddings (HyDE)**: Generate hypothetical documents to align the semantic space proposed in the paper [Precise Zero-Shot Dense Retrieval without Relevance Labels](https://arxiv.org/pdf/2212.10496.pdf)\n",
    "- **Step-Back Prompting**: A prompting technique that allows the LLM to do abstractions to derive high-level concepts based on the paper [Take A Step Back: Evoking Reasoning via Abstraction in Large Language Models](https://arxiv.org/pdf/2310.06117)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup libraries and environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install python-dotenv llama-index==0.10.33 llama-index-llms-openai==0.1.19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from util.helpers import get_wiki_pages, create_and_save_wiki_md_files\n",
    "from typing import Optional\n",
    "\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.core import (\n",
    "    SimpleDirectoryReader,\n",
    "    VectorStoreIndex,\n",
    "    PromptTemplate,\n",
    "    Settings,\n",
    ")\n",
    "from llama_index.core.prompts.prompt_type import PromptType\n",
    "from llama_index.core.prompts.base import BasePromptTemplate\n",
    "from llama_index.core.prompts.mixin import PromptDictType\n",
    "from llama_index.core.query_engine import CustomQueryEngine, TransformQueryEngine\n",
    "from llama_index.core.schema import QueryBundle\n",
    "from llama_index.core.indices.query.query_transform.base import BaseQueryTransform\n",
    "from llama_index.core.service_context_elements.llm_predictor import LLMPredictorType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "model = \"gpt-4o\"\n",
    "llm = OpenAI(api_key=OPENAI_API_KEY, model=model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rewrite-Retrieve-Read\n",
    "\n",
    "This techniques improves on the baseline RAG setup by adding and intermediate between the input and retriever. This step uses an LLM to filter and rephrase the input query before passing it to the retriever. This potentially allows the for multiple queries to be generated (representing multiple questions in the input) which can be be be sent to the retriever separately.\n",
    "\n",
    "In the following example we have two different knowledge sources that contain information about different subjects. For the sake of this example, we will just use two different wikipedia articles as the base for each knowledge source. This can also be seen as a form for **Query Routing**.\n",
    "\n",
    "Lets start by loading the data and the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "van_gogh_pages = get_wiki_pages([\"Vincent van Gogh\"])\n",
    "amsterdam_pages = get_wiki_pages([\"Amsterdam\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_and_save_wiki_md_files(van_gogh_pages, path=\"./data/docs/vangogh/\")\n",
    "create_and_save_wiki_md_files(amsterdam_pages, path=\"./data/docs/amsterdam/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "van_gogh_docs = SimpleDirectoryReader(\"./data/docs/vangogh\").load_data()\n",
    "amsterdam_docs = SimpleDirectoryReader(\"./data/docs/amsterdam\").load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vg_index = VectorStoreIndex.from_documents(documents=van_gogh_docs)\n",
    "a_index = VectorStoreIndex.from_documents(documents=amsterdam_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will create a custom pipeline that will use the LLM to rewrite the query before passing it to the retriever. This pipeline will have the following steps:\n",
    "- **Rewriter**: This step will use the LLM to rewrite the query into multiple questions an categorize for each question the knowledge source that should be used.\n",
    "- **Retriever**: This step will use the retriever to retrieve the documents for each question.\n",
    "- **Reader**: Answer each question using the retrieved documents for that document.\n",
    "- **AnswerMerger**: This step will merge the answers from the reader into a single answer and validate the answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "from llama_index.core.retrievers import BaseRetriever\n",
    "\n",
    "rewrite_prompt_template = PromptTemplate(\n",
    "    \"\"\"You're a helpful AI assistant that helps people learn about different topics. \n",
    "Given the following query: \n",
    "-----------------------------------\n",
    "{query_str},\n",
    "-----------------------------------\n",
    "\n",
    "Extract each question from the query and categorize it into one of the following categories separated into key and description pairs:\n",
    "-----------------------------------\n",
    "{categories_with_descriptions}\n",
    "-----------------------------------\n",
    "\n",
    "Your output should a comma separated list of questions with their corresponding category prepended in square brackets.\n",
    "Example: \n",
    "-----------------------------------\n",
    "\"What is the capital of France? And who is Prince\" -> \"[Geography]What is the capital of France?,[People]Who is Prince?\"\n",
    "-----------------------------------\n",
    "Answer:\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "qa_prompt = PromptTemplate(\n",
    "    \"\"\"You are a helpful assistant that answers questions. \n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Context: \n",
    "-----------------------------------\n",
    "{context}\n",
    "-----------------------------------\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "validate_prompt = PromptTemplate(\n",
    "    \"\"\"You are a helpful AI assistant that validates, corrects and combines information in answers to a query\n",
    "Query:\n",
    "-----------------------------------\n",
    "{query_stry}\n",
    "-----------------------------------\n",
    "\n",
    "Answers:\n",
    "-----------------------------------\n",
    "{answers}\n",
    "-----------------------------------\n",
    "\n",
    "Validate, correct and combine the answers to provide a single coherent response.\n",
    "Answer:    \n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "\n",
    "class RewriteRetrieveReadQueryEngine(CustomQueryEngine):\n",
    "    \"\"\"RAG String Query Engine.\"\"\"\n",
    "\n",
    "    categories: list[str]\n",
    "    descriptions: list[str]\n",
    "    retrievers: Dict[str, BaseRetriever]\n",
    "    llm: OpenAI = OpenAI(api_key=OPENAI_API_KEY, model=\"gpt-4o\")\n",
    "    verbose: bool = False\n",
    "\n",
    "    def custom_query(self, query_str: str):\n",
    "        categories_with_descriptions = \"\\n\".join(\n",
    "            [\n",
    "                f\"{category} - {description}\"\n",
    "                for category, description in zip(self.categories, self.descriptions)\n",
    "            ]\n",
    "        )\n",
    "        rewrite_prompt = rewrite_prompt_template.format(\n",
    "            query_str=query_str,\n",
    "            categories_with_descriptions=categories_with_descriptions,\n",
    "        )\n",
    "        rewrite_res = self.llm.complete(rewrite_prompt)\n",
    "            \n",
    "        questions = str(rewrite_res).replace(\"\\\"\", \"\").split(\",\")\n",
    "        if self.verbose:\n",
    "            print (\"Questions:\", questions)\n",
    "            \n",
    "        answers = []\n",
    "        for question in questions:\n",
    "            category, q = question[1:].split(\"]\")\n",
    "            if self.verbose:\n",
    "                print(\"\\n\\nRetrieving answer for question:\", q)\n",
    "                print(\"Using category:\", category)\n",
    "            nodes = self.retrievers[category].retrieve(q)\n",
    "            if self.verbose:\n",
    "                print(\"Retrieved nodes:\", nodes)\n",
    "            context = \"\\n\\n\".join([n.node.get_content() for n in nodes])\n",
    "            answer = self.llm.complete(\n",
    "                qa_prompt.format(question=q, context=context)\n",
    "            )\n",
    "            if self.verbose:\n",
    "                print(\"Answer:\", answer)\n",
    "            answers.append(answer.text)\n",
    "        if self.verbose:\n",
    "            print(\"\\n\\nValidating answers\")\n",
    "        response = self.llm.complete(\n",
    "            validate_prompt.format(query_stry=query_str, answers=\"\\n\".join(answers))\n",
    "        )\n",
    "\n",
    "        return str(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = [\"VAN_GOGH\", \"AMSTERDAM\"]\n",
    "descriptions = [\"Questions about Vincent van Gogh\", \"Questions about Amsterdam\"]\n",
    "retrievers = {\"VAN_GOGH\": vg_index.as_retriever(similarity_top_k=2), \"AMSTERDAM\": a_index.as_retriever(similarity_top_k=2)}\n",
    "\n",
    "query_engine = RewriteRetrieveReadQueryEngine(\n",
    "    categories=categories,\n",
    "    descriptions=descriptions,\n",
    "    retrievers=retrievers,\n",
    "    llm=llm,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "Hello, I am a student who is interested in learning about art and history. I have two questions that I would like to know more about.\n",
    "Who is Vincent van Gogh and what is Amsterdam famous for?\n",
    "\n",
    "Kind regards, Billy\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = query_engine.query(query)\n",
    "print(\"Response:\", response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypothetical Document Embeddings (HyDE)\n",
    "\n",
    "This technique generates hypothetical documents to align the semantic space of the retriever and the LLM. This allows the LLM to generate queries that are more suitable for the retrieval phase. It is based on the paper [Precise Zero-Shot Dense Retrieval without Relevance Labels](https://arxiv.org/pdf/2212.10496.pdf).\n",
    "\n",
    "This method can also be seen as a kind of **Query Expansion**, since it expands the original query with hypothetical documents.\n",
    "\n",
    "The method is split into four steps:\n",
    "1. Generate hypothetical documents based on the query using an LLM. These documents may contain incorrect information, but the idea is that they are likely to resemble the relevant documents that needs to be retrieved.\n",
    "2. Vectorize the hypothetical documents using an encoder which filters noise fom the hypothetical documents.\n",
    "3. Compute the average of the vectorized hypothetical documents to get the hypothetical document embedding.\n",
    "    - Optionally the average vector might include the embedding of the original query.\n",
    "4. Retrieve documents using the generated embedding.\n",
    "\n",
    "The following is the default prompt for the HyDE method:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HYDE_TMPL = (\n",
    "    \"Please write a passage to answer the question\\n\"\n",
    "    \"Try to include as many key details as possible.\\n\"\n",
    "    \"-----------------------------------\\n\"\n",
    "    \"{context_str}\\n\"\n",
    "    \"-----------------------------------\\n\"\n",
    "    'Passage:\"\"\"\\n'\n",
    ")\n",
    "\n",
    "DEFAULT_HYDE_PROMPT = PromptTemplate(HYDE_TMPL, prompt_type=PromptType.SUMMARY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A possible benefit of this method is that the prompt can be changed to better suit the specific use case. For example, if the retrieval phase is based on a specific domain, the prompt can be changed to better suit that domain.\n",
    "\n",
    "- **Fetch from scientific papers**: *Write a scientific paper to answer the following question:*\n",
    "- **Fetch from legal documents**: *Write a legal document to answer the following question:*\n",
    "- **Fetch from different languages**: *Write passages in Danish to answer the following question:*\n",
    "\n",
    "HyDE is implemented in LlamaIndex (including other frameworks) as `HyDEQueryTransform`. The following is the class implementation of this in LlamaIndex with some added print statements to better understand the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HyDEQueryTransform(BaseQueryTransform):\n",
    "    \"\"\"Hypothetical Document Embeddings (HyDE) query transform.\n",
    "\n",
    "    It uses an LLM to generate hypothetical answer(s) to a given query,\n",
    "    and use the resulting documents as embedding strings.\n",
    "\n",
    "    As described in `[Precise Zero-Shot Dense Retrieval without Relevance Labels]\n",
    "    (https://arxiv.org/abs/2212.10496)`\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        llm: Optional[LLMPredictorType] = None,\n",
    "        hyde_prompt: Optional[BasePromptTemplate] = None,\n",
    "        include_original: bool = True,\n",
    "        verbose: Optional[bool] = False,\n",
    "    ) -> None:\n",
    "        \"\"\"Initialize HyDEQueryTransform.\n",
    "\n",
    "        Args:\n",
    "            llm_predictor (Optional[LLM]): LLM for generating\n",
    "                hypothetical documents\n",
    "            hyde_prompt (Optional[BasePromptTemplate]): Custom prompt for HyDE\n",
    "            include_original (bool): Whether to include original query\n",
    "                string as one of the embedding strings\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self._llm = llm or Settings.llm\n",
    "        self._hyde_prompt = hyde_prompt or DEFAULT_HYDE_PROMPT\n",
    "        self._include_original = include_original\n",
    "        self._verbose = verbose\n",
    "\n",
    "    def _get_prompts(self) -> PromptDictType:\n",
    "        \"\"\"Get prompts.\"\"\"\n",
    "        return {\"hyde_prompt\": self._hyde_prompt}\n",
    "\n",
    "    def _update_prompts(self, prompts: PromptDictType) -> None:\n",
    "        \"\"\"Update prompts.\"\"\"\n",
    "        if \"hyde_prompt\" in prompts:\n",
    "            self._hyde_prompt = prompts[\"hyde_prompt\"]\n",
    "\n",
    "    def _run(self, query_bundle: QueryBundle, metadata: Dict) -> QueryBundle:\n",
    "        \"\"\"Run query transform.\"\"\"\n",
    "        query_str = query_bundle.query_str\n",
    "        hypothetical_doc = self._llm.predict(self._hyde_prompt, context_str=query_str)\n",
    "        if self._verbose:\n",
    "            print(f\"HyDEQueryTransform: hypothetical_doc={str(hypothetical_doc)}\")\n",
    "        embedding_strs = [hypothetical_doc]\n",
    "        if self._include_original:\n",
    "            embedding_strs.extend(query_bundle.embedding_strs)\n",
    "        return QueryBundle(\n",
    "            query_str=query_str,\n",
    "            custom_embedding_strs=embedding_strs,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = VectorStoreIndex.from_documents(documents=van_gogh_docs + amsterdam_docs)\n",
    "query_engine = index.as_query_engine(llm=llm)\n",
    "\n",
    "hyde = HyDEQueryTransform(llm=llm, include_original=True, verbose=True)\n",
    "hyde_query_engine = TransformQueryEngine(query_engine, hyde)\n",
    "\n",
    "base_response = query_engine.query(\"What are the best places to visit in Amsterdam?\")\n",
    "\n",
    "print(\"Base response:\", base_response)\n",
    "print(\"-\" * 100)\n",
    "\n",
    "hyde_response = hyde_query_engine.query(\"What are the best places to visit in Amsterdam?\")\n",
    "print(\"-\" * 100)\n",
    "print(\"HyDE response:\", hyde_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step-Back Prompting\n",
    "\n",
    "This technique allows the LLM to do abstractions to derive high-level concepts. This is done by prompting an LLM to take a step back and think about the question in a more abstract way. It is based on the paper [Take A Step Back: Evoking Reasoning via Abstraction in Large Language Models](https://arxiv.org/pdf/2310.06117).\n",
    "The idea is to define more abstract \"step-back problems\" based on the original problem. The LLM benefits from this approach when for example the query contains a lot of details or is very specific.\n",
    "\n",
    "Step-back prompting is more of a general prompt engineering technique, and can be used in many different contexts. \n",
    "Other prompt engineering techniques include:\n",
    "- **[Few-shot Prompting](https://www.promptingguide.ai/techniques/fewshot)** - Prompts that provide a few examples to enable context-learning\n",
    "- **[Chain-of-Thought](https://www.promptingguide.ai/techniques/cot)** - Prompts that guide the LLM through a chain of thoughts to reach a conclusion. \n",
    "- **[Self-Consistency](https://www.promptingguide.ai/techniques/consistency)** - Prompts that require the LLM to be consistent with previous answers\n",
    "- [Among others](https://www.promptingguide.ai/techniques)... \n",
    "\n",
    "\n",
    "The following is an example of how it can be used in the context of RAG with a mix of Step-back and few-shot prompting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STEP_BACK_PROMPT_TMPL = PromptTemplate(\n",
    "    \"\"\"\n",
    "    You are an expert at world knowledge. \n",
    "    Your task is to step back and paraphrase a question to a more generic step-back question, which is easier to answer. \n",
    "    \n",
    "    Here are a few examples:\n",
    "    \n",
    "    Human: Could the members of The Police perform lawful arrests?\n",
    "    AI: what can the members of The Police do?\n",
    "    \n",
    "    Human: Jan Sindel’s was born in what country?\n",
    "    AI: what is Jan Sindel’s personal history? \n",
    "    \n",
    "    Question: {query_str}\n",
    "    Step-back question:\"\"\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how Few-shot is used to give the LLM examples of how to abstract the questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "RESULT_PROMPT_TMPL = PromptTemplate(\n",
    "    \"\"\"You are an expert of world knowledge. \n",
    "I am going to ask you a question. \n",
    "Your response should be comprehensive and not contradicted with the following context if they are relevant. \n",
    "Otherwise, ignore them if they are not relevant.\n",
    "\n",
    "{normal_context}\n",
    "{step_back_context}\n",
    "\n",
    "Original Question: {query_str}\n",
    "Answer:\"\"\"\n",
    ")\n",
    "\n",
    "class StepBackQueryEngine(CustomQueryEngine):\n",
    "    retriever: BaseRetriever\n",
    "    llm: Optional[OpenAI] = OpenAI(api_key=OPENAI_API_KEY, model=\"gpt-4o\")\n",
    "    verbose: Optional[bool] = False\n",
    "    include_original: Optional[bool] = True\n",
    "\n",
    "    def custom_query(self, query_str: str):\n",
    "        \n",
    "        step_back_prompt = STEP_BACK_PROMPT_TMPL.format(query_str=query_str)\n",
    "        step_back_res = self.llm.complete(step_back_prompt)\n",
    "        if self.verbose:\n",
    "            print(\"\\n\\nStep-back Question:\\n\\n\", step_back_res, \"\\n\\n\")\n",
    "        step_back_context = self.retriever.retrieve(step_back_res.text)\n",
    "        if self.verbose:\n",
    "            print(\"\\n\\nStep-back Context:\\n\\n\", step_back_context, \"\\n\\n\")\n",
    "        \n",
    "        if self.include_original:\n",
    "            normal_context = self.retriever.retrieve(query_str)\n",
    "            if self.verbose:\n",
    "                print(\"\\n\\nNormal Context:\\n\\n\", normal_context, \"\\n\\n\")\n",
    "        else:\n",
    "            normal_context = []\n",
    "        \n",
    "        result_prompt = RESULT_PROMPT_TMPL.format(\n",
    "            normal_context=\"\\n\".join([n.node.get_content() for n in normal_context]),\n",
    "            step_back_context=\"\\n\".join([n.node.get_content() for n in step_back_context]),\n",
    "            query_str=query_str,\n",
    "        )\n",
    "        response = self.llm.complete(result_prompt)        \n",
    "        \n",
    "        return str(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = VectorStoreIndex.from_documents(documents=van_gogh_docs + amsterdam_docs)\n",
    "query_engine = StepBackQueryEngine(retriever=index.as_retriever(similarity_top_k=2), llm=llm, verbose=True)\n",
    "\n",
    "response = query_engine.query(\"What are the best places to visit in Amsterdam?\")\n",
    "print(str(response))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
