{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieval\n",
    "\n",
    "In baseline RAG, retrieval is usually done simply by using vector search. However, as the field has developed, researchers have discovered multiple ways to enhance the retrieval process. This notebook will cover the following retrieval methods: \n",
    "\n",
    "- **Iterative Retrieval** - Use the reasoning capabilities of LLMs to perform iterative retrieval-generation cycles until passing an evaluation step.\n",
    "- **Recursive-Retrieval** - Search for smaller documents and use chunk references or metadata references to retrieve the full document.\n",
    "- **Generator-Enhanced Retrieval** - Using a LLM to predict when and what to retrieve across generation with a process called [**F**orward-**L**ooking **A**ctive **RE**trieval augmented\n",
    "generation (FLARE)](https://arxiv.org/pdf/2305.06983)\n",
    "- **GraphRAG** - Using existing or LLM-generated knowledge graphs to enhance retrieval by harnessing the relationships between entities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup libraries and environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install python-dotenv\n",
    "%pip install llama-index==0.10.33\n",
    "%pip install llama-index-llms-openai==0.1.16\n",
    "%pip install llama-index-graph-stores-nebula==0.1.2\n",
    "%pip install llama-index-readers-wikipedia==0.1.4\n",
    "%pip install llama-index-readers-papers=0.1.4\n",
    "%pip install llama-index-readers-web==0.1.12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import copy\n",
    "import json\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from util.helpers import (\n",
    "    get_malazan_pages,\n",
    "    create_and_save_md_files,\n",
    ")\n",
    "\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding, OpenAIEmbeddingModelType\n",
    "from llama_index.core import (\n",
    "    SimpleDirectoryReader,\n",
    "    VectorStoreIndex,\n",
    "    download_loader,\n",
    "    KnowledgeGraphIndex,\n",
    "    PromptTemplate,\n",
    ")\n",
    "from llama_index.core.query_engine import (\n",
    "    FLAREInstructQueryEngine,\n",
    "    RetrieverQueryEngine,\n",
    "    RetryQueryEngine,\n",
    "    RetryGuidelineQueryEngine,\n",
    ")\n",
    "\n",
    "from llama_index.readers.web import SimpleWebPageReader\n",
    "from llama_index.readers.papers import ArxivReader\n",
    "from llama_index.readers.wikipedia import WikipediaReader\n",
    "from llama_index.graph_stores.nebula import NebulaGraphStore\n",
    "from llama_index.readers.wikipedia import WikipediaReader\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.core.schema import IndexNode\n",
    "from llama_index.core.extractors import (\n",
    "    SummaryExtractor,\n",
    "    QuestionsAnsweredExtractor,\n",
    ")\n",
    "from llama_index.core.retrievers import RecursiveRetriever\n",
    "from llama_index.core.evaluation import (\n",
    "    RelevancyEvaluator,\n",
    "    GuidelineEvaluator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: This is ONLY necessary in jupyter notebook.\n",
    "# Details: Jupyter runs an event-loop behind the scenes.\n",
    "#          This results in nested event-loops when we start an event-loop to make async queries.\n",
    "#          This is normally not allowed, we use nest_asyncio to allow it for convenience.\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "llm = OpenAI(api_key=OPENAI_API_KEY, model=\"gpt-4-turbo\")\n",
    "embed_model = OpenAIEmbedding(api_key=OPENAI_API_KEY, model=OpenAIEmbeddingModelType.TEXT_EMBED_3_SMALL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read papers from arXiv as documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = ArxivReader()\n",
    "reader.load_data\n",
    "\n",
    "papers = [\"2404.10981\", \"2305.06983\", \"2312.10997\"]\n",
    "papers_dir = \"./data/docs/arxiv\"\n",
    "\n",
    "arxiv_res = [reader.load_papers_and_abstracts(search_query=f\"id:{paper}\", max_results=1, papers_dir=papers_dir) for paper in papers]\n",
    "documents = [doc for sublist in [d for (d, _) in arxiv_res] for doc in sublist]\n",
    "abstracts = [a[0] for (_, a) in arxiv_res]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "papers_index = VectorStoreIndex.from_documents(documents=documents, show_progress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iterative Retrieval\n",
    "In **Iterative Retrieval**, the key part of the process is the evaluator or *judge* that helps self-correct the retrieval process. \n",
    "\n",
    "The pipeline performs the following steps:\n",
    "1. first queries the base query engine, then\n",
    "2. use the evaluator to decided if the response passes.\n",
    "3. If the response passes, then return response,\n",
    "4. Otherwise, transform the original query with the evaluation result (query, response, and feedback) into a new query,\n",
    "5. Repeat up to max_retries\n",
    "\n",
    "There's different types of evaluators that can be used, depending on the use-case. Sometimes it might be useful to evaluate the answer specifically, or the context. Other times you might need to create some guidelines for the LLM to use in order to evaluate whether to perform another retrieval step or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Prompt of RelevancyEvaluator\n",
    "DEFAULT_EVAL_TEMPLATE = PromptTemplate(\n",
    "    \"Your task is to evaluate if the response for the query \\\n",
    "    is in line with the context information provided.\\n\"\n",
    "    \"You have two options to answer. Either YES/ NO.\\n\"\n",
    "    \"Answer - YES, if the response for the query \\\n",
    "    is in line with context information otherwise NO.\\n\"\n",
    "    \"Query and Response: \\n {query_str}\\n\"\n",
    "    \"Context: \\n {context_str}\\n\"\n",
    "    \"Answer: \"\n",
    ")\n",
    "\n",
    "DEFAULT_REFINE_TEMPLATE = PromptTemplate(\n",
    "    \"We want to understand if the following query and response is\"\n",
    "    \"in line with the context information: \\n {query_str}\\n\"\n",
    "    \"We have provided an existing YES/NO answer: \\n {existing_answer}\\n\"\n",
    "    \"We have the opportunity to refine the existing answer \"\n",
    "    \"(only if needed) with some more context below.\\n\"\n",
    "    \"------------\\n\"\n",
    "    \"{context_msg}\\n\"\n",
    "    \"------------\\n\"\n",
    "    \"If the existing answer was already YES, still answer YES. \"\n",
    "    \"If the information is present in the new context, answer YES. \"\n",
    "    \"Otherwise answer NO.\\n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = RelevancyEvaluator(llm=llm)\n",
    "query_engine = RetryQueryEngine(query_engine=papers_index.as_query_engine(), evaluator=evaluator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = GuidelineEvaluator(llm=llm, guidelines=\"List elements of the answer as bullets\")\n",
    "query_engine = RetryGuidelineQueryEngine(query_engine=papers_index.as_query_engine(), resynthesize_query=True, guideline_evaluator=evaluator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What are the phases of Advanced RAG?\"\n",
    "response = query_engine.query(str_or_query_bundle=query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response.metadata)\n",
    "print(str(response))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recursive-Retrieval\n",
    "\n",
    "In **Recursive-Retrieval**, we search to relevant documents using references between smaller documents and their related larger documents. This enables us to optimize the narrowing the search space for the first retrieval, and then recursively increasing the context using the references.\n",
    "\n",
    "We distinguish between two types of recursive-retrieval:\n",
    "1. **Chunk-References** - References between smaller and larger documents.\n",
    "2. **Metadata-References** - References between metadata like summaries or generated questions to documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chunk References"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = WikipediaReader()\n",
    "wiki_documents = reader.load_data(pages=[\"Vincent Van Gogh\"])\n",
    "wiki_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "node_parser = SentenceSplitter(chunk_size=1024)\n",
    "base_nodes = node_parser.get_nodes_from_documents(documents=wiki_documents, show_progress=True)\n",
    "# set node ids to be a constant\n",
    "for idx, node in enumerate(base_nodes):\n",
    "    node.id_ = f\"node-{idx}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_chunk_sizes = [256, 512]\n",
    "sub_node_parsers = [\n",
    "    SentenceSplitter(chunk_size=c, chunk_overlap=20) for c in sub_chunk_sizes\n",
    "]\n",
    "\n",
    "all_nodes = []\n",
    "for base_node in base_nodes:\n",
    "    for n in sub_node_parsers:\n",
    "        sub_nodes = n.get_nodes_from_documents([base_node])\n",
    "        sub_inodes = [\n",
    "            IndexNode.from_text_node(sn, base_node.node_id) for sn in sub_nodes\n",
    "        ]\n",
    "        all_nodes.extend(sub_inodes)\n",
    "\n",
    "    # also add original node to node\n",
    "    original_node = IndexNode.from_text_node(base_node, base_node.node_id)\n",
    "    all_nodes.append(original_node)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test the effectiveness of chunk references go to \"**Test recursive-retrieval engine**\" section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metadata References"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extractors = [\n",
    "    SummaryExtractor(summaries=[\"self\"], llm=OpenAI(api_key=OPENAI_API_KEY, model=\"gpt-3.5-turbo\"), show_progress=True),\n",
    "    QuestionsAnsweredExtractor(questions=5, llm=OpenAI(api_key=OPENAI_API_KEY, model=\"gpt-3.5-turbo\"), show_progress=True),\n",
    "]\n",
    "# run metadata extractor across base nodes, get back dictionaries\n",
    "node_to_metadata = {}\n",
    "for extractor in extractors:\n",
    "    metadata_dicts = extractor.extract(base_nodes)\n",
    "    for node, metadata in zip(base_nodes, metadata_dicts):\n",
    "        if node.node_id not in node_to_metadata:\n",
    "            node_to_metadata[node.node_id] = metadata\n",
    "        else:\n",
    "            node_to_metadata[node.node_id].update(metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_metadata_dicts(path, data):\n",
    "    with open(path, \"w\") as fp:\n",
    "        json.dump(data, fp)\n",
    "\n",
    "\n",
    "def load_metadata_dicts(path):\n",
    "    with open(path, \"r\") as fp:\n",
    "        data = json.load(fp)\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "path = \"./data/retrieval\"\n",
    "if not os.path.exists(path):\n",
    "    print(\"Creating directory: \", path)\n",
    "    os.makedirs(path)\n",
    "\n",
    "save_metadata_dicts(path + \"/metadata_dicts.json\", node_to_metadata)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_dicts = load_metadata_dicts(\"./data/retrieval/metadata_dicts.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_nodes = copy.deepcopy(base_nodes)\n",
    "for node_id, metadata in node_to_metadata.items():\n",
    "    print(node_id)\n",
    "    for val in metadata.values():\n",
    "        all_nodes.append(IndexNode(text=val, index_id=node_id))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test recursive-retrieval engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "all_nodes_dict = {n.node_id: n for n in all_nodes}\n",
    "recursive_index = VectorStoreIndex(all_nodes, embed_model=embed_model, show_progress=True)\n",
    "retriever = RecursiveRetriever(\n",
    "    \"vector\",\n",
    "    retriever_dict={\"vector\": recursive_index.as_retriever(similarity_top_k=2)},\n",
    "    node_dict=all_nodes_dict,\n",
    "    verbose=True,\n",
    ")\n",
    "recursive_query_engine = RetrieverQueryEngine.from_args(retriever=retriever, llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = recursive_query_engine.query(\"What are the phases of Advanced RAG?\")\n",
    "print(response.metadata)\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generator-Enhanced Retrieval\n",
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GraphRAG\n",
    "TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
