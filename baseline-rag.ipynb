{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fcba9c38-ce3e-486a-8d3a-96e3a37b70ea",
   "metadata": {},
   "source": [
    "# Baseline RAG example\n",
    "\n",
    "This is a simple example of a baseline RAG application which purpose is to answer questions about the fantasy series [Malazan Universe](https://malazan.fandom.com/wiki/Malazan_Wiki) created by Steven Erikson and Ian C. Esslemont.\n",
    "\n",
    "First the example will show each step of a baseline RAG pipeline including **Indexing**, **Retrieval** and **Generation**. This is done in order to show the architecture without the abstraction provided by frameworks like LlamaIndex and LangChain.\n",
    "Then a more \"normal\" example will be shown using LlamaIndex.\n",
    "\n",
    "As a vector database, we will use [ChromaDB](https://docs.trychroma.com/), but this can easily be exchanged with other databases.\n",
    "\n",
    "In this example, we will use the following technologies\n",
    "\n",
    "- OpenAI API\n",
    "- ChromaDB\n",
    "- LlamaIndex\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae217db9-221f-4811-9101-c87d3db2c821",
   "metadata": {},
   "source": [
    "### Setup libraries and environment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8edf740a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install python-dotenv openai==1.25.0 mdutils==1.6.0 fandom-py==0.2.1  chromadb==0.5.0 llama-index==0.10.33 llama-index-llms-openai==0.1.16 llama-index-vector-stores-chroma==0.1.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d7a26f5a-d799-4b80-93e5-60579a74852e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import chromadb\n",
    "import chromadb.utils.embedding_functions as embedding_functions\n",
    "\n",
    "from chromadb import Settings\n",
    "from openai import OpenAI\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.core import SimpleDirectoryReader, PromptTemplate\n",
    "\n",
    "from util.helpers import get_malazan_pages, create_and_save_md_files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "772c0dca-fd51-4348-9ada-8ca12d0139bc",
   "metadata": {},
   "source": [
    "### Environment variables\n",
    "\n",
    "For this example you need to use an OpenAI API key. Go to [your API keys](https://platform.openai.com/api-keys) in the OpenAI console to generate one.\n",
    "\n",
    "Then add the following to a `.env` file in the root of the project.\n",
    "\n",
    "```\n",
    "OPENAI_API_KEY=<YOUR_KEY_HERE>\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79b5d76d-dae0-4cfe-b9f1-1de2477b465a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1224aade",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "openai_ef = embedding_functions.OpenAIEmbeddingFunction(\n",
    "    api_key=OPENAI_API_KEY,\n",
    "    model_name=\"text-embedding-3-small\"\n",
    ")\n",
    "\n",
    "chroma_client = chromadb.PersistentClient(\n",
    "    path=\"./data/baseline-rag/chromadb\", settings=Settings(allow_reset=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7f0eacc-5118-4dbb-a22f-63769c527184",
   "metadata": {},
   "source": [
    "## Fetch documents and save them as markdown files\n",
    "\n",
    "Here we fetch pages from the Fandom Malazan Wiki. These are the documents that we will use as our \"knowledge base\" in order to supply context to our prompts.\n",
    "\n",
    "We also pre-process the content in order to be able to add them to our vector database.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a72c1c81-1257-4e99-b252-f3315b868b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pages = get_malazan_pages()\n",
    "create_and_save_md_files(pages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae6f25f-66b0-4acb-8503-1b00c2868c20",
   "metadata": {},
   "source": [
    "## Indexing\n",
    "\n",
    "In this step, we will index the documents in our vector database. This will allow us to retrieve the most relevant documents when we ask a question.\n",
    "\n",
    "We will use ChromaDB as our vector database and 'text-embedding-3-small' from OpenAI as our embedding model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8534742",
   "metadata": {},
   "source": [
    "#### Fetch and process saved documents\n",
    "\n",
    "First we need to fetch the documents we saved earlier.\n",
    "\n",
    "Then we will process the documents in order to add them to our vector database.\n",
    "The `SimpleDirectoryReader` fetches each section of the markdown file\n",
    "Then each section is split in to smaller chunks of text and each chunk is embedded using the OpenAI API.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add2504e",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = SimpleDirectoryReader('./data/docs').load_data()\n",
    "text_splitter = SentenceSplitter(chunk_size=512, chunk_overlap=20)\n",
    "\n",
    "document_data = []\n",
    "\n",
    "for document in documents:\n",
    "    chunks = text_splitter.split_text(document.text)\n",
    "    for idx, chunk in enumerate(chunks):\n",
    "        embedding = openai_client.embeddings.create(\n",
    "            input=chunk, model=\"text-embedding-3-small\")\n",
    "        document_data.append({\n",
    "            \"id\": f\"{document.id_}-{idx}\",\n",
    "            \"text\": chunk,\n",
    "            \"metadata\": document.metadata,\n",
    "            \"embedding\": embedding.data[0].embedding\n",
    "        })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f9e5fcc",
   "metadata": {},
   "source": [
    "#### Add documents to ChromaDB\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69311d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [doc[\"text\"] for doc in document_data]\n",
    "embeddings = [doc[\"embedding\"] for doc in document_data]\n",
    "metadatas = [doc[\"metadata\"] for doc in document_data]\n",
    "ids = [doc[\"id\"] for doc in document_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d947dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "chroma_client.reset()\n",
    "collection = chroma_client.get_or_create_collection(\n",
    "    name=\"malazan\", metadata={\"hnsw:space\": \"cosine\"}, embedding_function=openai_ef)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d806a863",
   "metadata": {},
   "outputs": [],
   "source": [
    "collection.add(\n",
    "    embeddings=embeddings,\n",
    "    documents=documents,\n",
    "    metadatas=metadatas,\n",
    "    ids=ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "026a9edc",
   "metadata": {},
   "source": [
    "## Retrieval\n",
    "\n",
    "In this step, we will retrieve the most relevant documents to a given question. We will use the vector database to retrieve the most similar documents to the question.\n",
    "\n",
    "In order to do this we will use the `text-embedding-3-small` model (**the same model used to index the documents**) from OpenAI to embed the question and then use the vector database to retrieve the most similar documents.\n",
    "\n",
    "We will retrieve the top 5 documents based on the _cosine similarity_ between the question and the documents. Other similarity metrics can be used as well like squared L2 or inner product.\n",
    "\n",
    "Change `cosine` to `l2` or `ip` when creating the collection above to try these out.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2223843a",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What was the titles of Anomander Rake?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03a7813e",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Who is Tayschrenn?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b01cf6c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is Kurald Galain?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "156f5fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = collection.query(query_texts=[query], n_results=5)\n",
    "context = result[\"documents\"][0]\n",
    "context"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e7a6a0",
   "metadata": {},
   "source": [
    "## Generation\n",
    "\n",
    "In this step, we will generate an answer to the question using the retrieved documents as context. We will use the OpenAI API to generate the answer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a1f320",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate(\"\"\"You are a helpful assistant that answers questions about the Malazan Fantasy Universe using provided context. \n",
    "\n",
    "Question: {query}\n",
    "\n",
    "Context: \n",
    "{context}\n",
    "\"\"\")\n",
    "message = prompt.format(query=query, context=\"\\n\\n\".join(context))\n",
    "message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cffca2b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "stream = openai_client.chat.completions.create(\n",
    "    messages=[{\"role\": \"user\", \"content\": message}],\n",
    "    model=\"gpt-4-turbo\",\n",
    "    stream=True)\n",
    "\n",
    "for chunk in stream:\n",
    "    print(chunk.choices[0].delta.content or \"\", end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e1fad80",
   "metadata": {},
   "source": [
    "## Normal example using LlamaIndex\n",
    "\n",
    "In this example, we will use LlamaIndex to abstract the indexing and retrieval steps. This shows how easily the same pipeline can be implemented using LlamaIndex.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "356930ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "from chromadb import Settings\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.core import SimpleDirectoryReader, VectorStoreIndex\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.core.ingestion import IngestionPipeline\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "\n",
    "# ChromaDB Vector Store\n",
    "chroma_client = chromadb.PersistentClient(\n",
    "    path=\"./data/baseline-rag/chromadb\", settings=Settings(allow_reset=True))\n",
    "chroma_client.reset()\n",
    "collection = chroma_client.get_or_create_collection(\n",
    "    name=\"malazan\", metadata={\"hnsw:space\": \"cosine\"})\n",
    "vector_store = ChromaVectorStore(chroma_collection=collection)\n",
    "\n",
    "# OpenAI Embedding and LLM\n",
    "embedding = OpenAIEmbedding(api_key=OPENAI_API_KEY,\n",
    "                            model=\"text-embedding-3-small\")\n",
    "llm = OpenAI(api_key=OPENAI_API_KEY, model=\"gpt-4-turbo\")\n",
    "\n",
    "# Define the ingestion pipeline to add documents to vector store\n",
    "pipeline = IngestionPipeline(\n",
    "    transformations=[\n",
    "        SentenceSplitter(chunk_size=512, chunk_overlap=20),\n",
    "        embedding,\n",
    "    ],\n",
    "    vector_store=vector_store,\n",
    ")\n",
    "\n",
    "# Create index with the vector store and using the embedding model\n",
    "index = VectorStoreIndex.from_vector_store(\n",
    "    vector_store=vector_store, embed_model=embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58348922",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch documents\n",
    "documents = SimpleDirectoryReader('./data/docs').load_data()\n",
    "\n",
    "# Run pipeline\n",
    "pipeline.run(documents=documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1afa9720",
   "metadata": {},
   "source": [
    "#### Create base QueryEngine from LlamaIndex\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79604a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = index.as_query_engine(llm=llm, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3f4a4ed",
   "metadata": {},
   "source": [
    "#### Or alternatively, create a CustomQueryEngine\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9f60384",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import PromptTemplate\n",
    "from llama_index.core.query_engine import CustomQueryEngine\n",
    "from llama_index.core.retrievers import BaseRetriever\n",
    "from llama_index.core import get_response_synthesizer\n",
    "from llama_index.core.response_synthesizers import BaseSynthesizer\n",
    "\n",
    "qa_prompt = PromptTemplate(\n",
    "    \"\"\"You are a helpful assistant that answers questions about the Malazan Fantasy Universe using provided context.\n",
    "    Context information is below.\n",
    "    ---------------------\n",
    "    {context_str}\n",
    "    ---------------------\n",
    "    Given the context information and not prior knowledge, answer the query.\n",
    "    Query: {query_str}\n",
    "    Answer: \n",
    "    \"\"\",\n",
    ")\n",
    "\n",
    "\n",
    "class RAGQueryEngine(CustomQueryEngine):\n",
    "    \"\"\"RAG String Query Engine.\"\"\"\n",
    "\n",
    "    retriever: BaseRetriever\n",
    "    response_synthesizer: BaseSynthesizer\n",
    "    llm: OpenAI\n",
    "    qa_prompt: PromptTemplate\n",
    "\n",
    "    def custom_query(self, query_str: str):\n",
    "        nodes = self.retriever.retrieve(query_str)\n",
    "        context_str = \"\\n\\n\".join([n.node.get_content() for n in nodes])\n",
    "        print(\"Prompt:\\n\\n\", qa_prompt.format(\n",
    "            context_str=context_str, query_str=query_str))\n",
    "        response = self.llm.complete(\n",
    "            qa_prompt.format(context_str=context_str, query_str=query_str)\n",
    "        )\n",
    "\n",
    "        return str(response)\n",
    "\n",
    "\n",
    "synthesizer = get_response_synthesizer(response_mode=\"compact\")\n",
    "query_engine = RAGQueryEngine(\n",
    "    retriever=index.as_retriever(),\n",
    "    response_synthesizer=synthesizer,\n",
    "    llm=llm,\n",
    "    qa_prompt=qa_prompt,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab9d8ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = query_engine.query(query)\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ce99b7d",
   "metadata": {},
   "source": [
    "## Simplest RAG implementation using LlamaIndex\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cceb724b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch documents\n",
    "documents = SimpleDirectoryReader('./data/docs').load_data()\n",
    "\n",
    "# build VectorStoreIndex that takes care of chunking documents\n",
    "# and encoding chunks to embeddings for future retrieval\n",
    "index = VectorStoreIndex.from_documents(documents=documents)\n",
    "\n",
    "# The QueryEngine class is equipped with the generator\n",
    "# and facilitates the retrieval and generation steps\n",
    "query_engine = index.as_query_engine()\n",
    "\n",
    "# Use your Default RAG\n",
    "response = query_engine.query(query)\n",
    "response"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
