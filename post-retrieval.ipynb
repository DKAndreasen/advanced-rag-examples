{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Post-retrieval processing\n",
    "\n",
    "In the \"Post-retrieval\" phase of RAG, the retrieved documents are processed to extract the relevant information. In order to optimize generation. \n",
    "\n",
    "The retrieval phase results in a list of documents. \n",
    "\n",
    "This notebook demonstrates three different techniques for post-retrieval processing:\n",
    "\n",
    "- Reranking\n",
    "- Compression\n",
    "- Fusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup libraries and environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install python-dotenv\n",
    "%pip install mdutils==1.6.0\n",
    "%pip install llama-index==0.10.33\n",
    "%pip install llama-index-llms-openai==0.1.16\n",
    "%pip install llama-index-postprocessor-rankgpt-rerank==0.1.3\n",
    "%pip install llama-index-postprocessor-cohere-rerank==0.1.4\n",
    "%pip install llama-index-postprocessor-longllmlingua==0.1.2\n",
    "%pip install llmlingua==0.2.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from util.helpers import get_wiki_pages, create_and_save_wiki_md_files\n",
    "\n",
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.core.schema import QueryBundle\n",
    "from llama_index.postprocessor.rankgpt_rerank import RankGPTRerank\n",
    "from llama_index.postprocessor.cohere_rerank import CohereRerank\n",
    "from llama_index.postprocessor.longllmlingua import LongLLMLinguaPostprocessor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add `COHERE_API_KEY` with API key for the Cohere API to `.env` file.\n",
    "Sign up for free and create one here: [Cohere Dashboard](https://dashboard.cohere.com/api-keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "COHERE_API_KEY = os.getenv(\"COHERE_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pages = get_wiki_pages([\"Vincent Van Gogh\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_and_save_wiki_md_files(pages=pages, path=\"./data/docs/wiki/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = SimpleDirectoryReader(\"./data/docs/wiki/\").load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(api_key=OPENAI_API_KEY, model=\"gpt-3.5-turbo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = VectorStoreIndex.from_documents(\n",
    "    documents=documents,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Describe the later life of Vincent Van Gogh.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reranking\n",
    "\n",
    "Since vectors are essentially compressions of the meeaning behind some text, there is a loss of information. So what do we do if relevant information is below top_k cutoff for ou retrieval? The simplest solution would be to increase the top_k value, but this would increase the computational cost. Another problem is that LLMs suffer from the \"Lost in the Middle\" phenomenon, where it usually focuses on the extremes of the input prompt. This means that its prudent to have the most relevant information at the top of the list.\n",
    "\n",
    "A solution to this problem is **reranking**. Reranking fundamentally reorders the documents chunks to highlight the most pertinent results first, effectively reducing the overall document pool, severing a dual purpose in information retrieval, acting as both an enhancer and a filter, delivering refined inputs for more precise language model processing.\n",
    "\n",
    "In this example we will see two approaches to reranking:\n",
    "- LLM reranking \n",
    "    - having a language model rerank the documents\n",
    "    - specifically, we will use RankGPT using ChatGPT from OpenAI\n",
    "- Ranking using Cohere Rerank3 - A managed reranking model by Cohere"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLMRerank"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The benefits of using a language model to rerank documents are that it can understand the context of the query and the documents, and can provide a more nuanced ranking.\n",
    "\n",
    "RankGPT uses the following prompt to rank the retrieved documents:\n",
    "```\n",
    "You are RankGPT, an intelligent assistant that can rank passages based on their relevancy to the query.\n",
    "\n",
    "I will provide you with {num} passages, each indicated by number identifier []. \n",
    "\n",
    "Rank the passages based on their relevance to query: {query}.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reranker = RankGPTRerank(llm=llm, top_n=3, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = index.as_query_engine(\n",
    "    similarity_top_k=10,\n",
    "    node_postprocessors=[reranker],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = query_engine.query(query)\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cohere\n",
    "\n",
    "In this example we will use [**Rerank 3**](https://cohere.com/blog/rerank-3), which is a managed reranking model by **Cohere** that can be used to rerank documents. It is a transformer model that is trained on a large dataset of queries and documents to rerank documents based on their relevance to the query.\n",
    "\n",
    "The model includes\n",
    "- 4k context length to significantly improve search quality for longer documents \n",
    "- Ability to search over multi-aspect and semi-structured data like emails, invoices, JSON documents, code, and tables\n",
    "- Multilingual coverage of 100+ languages \n",
    "\n",
    "Since it is closed source we can not go through the inner workings of the model, but the in many applications it has shown to be very effective at reducing latency and increasing accuracy of the generation step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reranker = CohereRerank(api_key=COHERE_API_KEY, top_n=3, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = index.as_query_engine(\n",
    "    similarity_top_k=20,\n",
    "    node_postprocessors=[reranker],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = query_engine.query(query)\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt Compression\n",
    "\n",
    "Prompt compression is the process of reducing the length of the prompt to focus on the most important information. This can be useful when the prompt is too long or contains irrelevant information. Its also an effective way to reduce the computational cost (reducing time and money spent) of the model as well as combat the \"Lost in the middle\" phenomenon.\n",
    "\n",
    "In this example we will be using **LLMLingua** developed by Microsoft Research ([original paper](https://arxiv.org/pdf/2310.05736)) to reduce the size of prompts, while keeping the information that is relevant to the query.\n",
    "The main idea behind LLMLingua is to use a smaller language model to calculate the mutual information between the prompt and the query and use this to perform prompt compression.\n",
    "\n",
    "More specifically we will be using a process called **LongLLMLingua**. This process starts by reordering the documents to have the most relevant information at the top. Then it uses LLMLingua to compress the prompt and finally uses the compressed prompt to generate the final output.\n",
    "\n",
    "Other methods of prompt compression include:\n",
    "- [Selective Context](https://arxiv.org/pdf/2304.12102)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_lingua_compressor = LongLLMLinguaPostprocessor(\n",
    "    instruction_str=\"Given the context, please answer the final question\",\n",
    "    target_token=300,\n",
    "    rank_method=\"longllmlingua\",\n",
    "    additional_compress_kwargs={\n",
    "        \"condition_compare\": True,\n",
    "        \"condition_in_question\": \"after\",\n",
    "        \"context_budget\": \"+100\",\n",
    "        \"reorder_context\": \"sort\",  # enable document reorder\n",
    "        \"dynamic_context_compression_ratio\": 0.4, # enable dynamic compression ratio\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = index.as_retriever(similarity_top_k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieved_nodes = retriever.retrieve(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_retrieved_nodes = llm_lingua_compressor.postprocess_nodes(\n",
    "    retrieved_nodes, query_bundle=QueryBundle(query_str=query)\n",
    ")\n",
    "original_contexts = \"\\n\\n\".join([n.get_content() for n in retrieved_nodes])\n",
    "compressed_contexts = \"\\n\\n\".join([n.get_content() for n in new_retrieved_nodes])\n",
    "\n",
    "original_tokens = llm_lingua_compressor._llm_lingua.get_token_length(original_contexts)\n",
    "compressed_tokens = llm_lingua_compressor._llm_lingua.get_token_length(compressed_contexts)\n",
    "\n",
    "print(\"Original Contexts:\")\n",
    "print(\"-------------------\")\n",
    "print(original_contexts)\n",
    "print(\"-------------------\")\n",
    "print(\"Compressed Contexts:\")\n",
    "print(\"-------------------\")\n",
    "print(compressed_contexts)\n",
    "print(\"-------------------\")\n",
    "print(\"Original Tokens:\", original_tokens)\n",
    "print(\"Compressed Tokens:\", compressed_tokens)\n",
    "print(\"Compressed Ratio:\", f\"{original_tokens/(compressed_tokens + 1e-5):.2f}x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = index.as_query_engine(\n",
    "    similarity_top_k=10,\n",
    "    node_postprocessors=[llm_lingua_compressor],\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
