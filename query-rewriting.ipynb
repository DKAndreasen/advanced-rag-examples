{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query Rewriting\n",
    "\n",
    "In RAG we can often encounter issues with the users query. This could be inaccuracy, ambiguity, or even a lack of information. In general its pretty common that the query is not optimal for the retrieval phase of RAG.\n",
    "\n",
    "Imagine the following scenario. You're building an application that automatically answers support tickets. Queries in this case will be emails from customers. You can imagine the following issues that can arise:\n",
    "\n",
    "- Emails often contain noise, such as greetings, signatures, etc. which are not relevant for the question and can confuse the model.\n",
    "- The user might have multiple questions in the same email, which makes it harder to retrieve the correct documents. \n",
    "- The question posed by the user might be ambiguous, or not specific enough.\n",
    "\n",
    "In many other applications, such as search engines, chatbots, etc. we can encounter similar issues. This notebook will explore different examples and techniques to rewrite queries to make them more suitable for the retrieval phase of RAG.\n",
    "\n",
    "The following exmaples will be covered:\n",
    "\n",
    "- **Rewrite-Retrieve-Read**: Explore a technique proposed in the paper [Query Rewriting for Retrieval-Augmented Large Language Models](https://arxiv.org/pdf/2305.14283.pdf)\n",
    "- **Hypothetical Document Embeddings (HyDE)**: Generate hypothetical documents to align the semantic space proposed in the paper [Precise Zero-Shot Dense Retrieval without Relevance Labels](https://arxiv.org/pdf/2212.10496.pdf)\n",
    "- **Step-Back Prompting**: A prompting technique that allows the LLM to do abstractions to derive high-level concepts based on the paper [Take A Step Back: Evoking Reasoning via Abstraction in Large Language Models](https://arxiv.org/pdf/2310.06117)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup libraries and environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install python-dotenv\n",
    "%pip install llama-index==0.10.33\n",
    "%pip install llama-index-llms-openai==0.1.16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from util.helpers import get_wiki_pages, create_and_save_wiki_md_files\n",
    "\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.core import SimpleDirectoryReader, VectorStoreIndex, PromptTemplate, get_response_synthesizer\n",
    "from llama_index.core.query_engine import CustomQueryEngine\n",
    "from llama_index.core.retrievers import BaseRetriever\n",
    "from llama_index.core.response_synthesizers import BaseSynthesizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "model = \"gpt-4-turbo\"\n",
    "llm = OpenAI(api_key=OPENAI_API_KEY, model=model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rewrite-Retrieve-Read\n",
    "\n",
    "This techniques improves on the baseline RAG setup by adding and intermediate between the input and retriever. This step uses an LLM to filter and rephrase the input query before passing it to the retriever. This potentially allows the for multiple queries to be generated (representing multiple questions in the input) which can be be be sent to the retriever separately.\n",
    "\n",
    "In the following example we have two different knowledge sources that contain information about different subjects. For the sake of this example, we will just use two different wikipedia articles as the base for each knowledge source.\n",
    "\n",
    "Lets start by loading the data and the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "van_gogh_pages = get_wiki_pages([\"Vincent van Gogh\"])\n",
    "amsterdam_pages = get_wiki_pages([\"Amsterdam\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_and_save_wiki_md_files(van_gogh_pages, path=\"./data/docs/vangogh/\")\n",
    "create_and_save_wiki_md_files(amsterdam_pages, path=\"./data/docs/amsterdam/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "van_gogh_docs = SimpleDirectoryReader(\"./data/docs/vangogh\").load_data()\n",
    "amsterdam_docs = SimpleDirectoryReader(\"./data/docs/amsterdam\").load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vg_index = VectorStoreIndex.from_documents(documents=van_gogh_docs)\n",
    "a_index = VectorStoreIndex.from_documents(documents=amsterdam_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will create a custom pipeline that will use the LLM to rewrite the query before passing it to the retriever. This pipeline will have the following steps:\n",
    "- **Rewriter**: This step will use the LLM to rewrite the query into multiple questions an categorize for each question the knowledge source that should be used.\n",
    "- **Retriever**: This step will use the retriever to retrieve the documents for each question.\n",
    "- **Reader**: Answer each question using the retrieved documents for that document.\n",
    "- **AnswerMerger**: This step will merge the answers from the reader into a single answer and validate the answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "from llama_index.core.retrievers import BaseRetriever\n",
    "\n",
    "rewrite_prompt_template = PromptTemplate(\n",
    "    \"\"\"You're a helpful AI assistant that helps people learn about different topics. \n",
    "Given the following query: \n",
    "-----------------------------------\n",
    "{query_str},\n",
    "-----------------------------------\n",
    "\n",
    "Extract each question from the query and categorize it into one of the following categories separated into key and description pairs:\n",
    "-----------------------------------\n",
    "{categories_with_descriptions}\n",
    "-----------------------------------\n",
    "\n",
    "Your output should a comma separated list of questions with their corresponding category prepended in square brackets.\n",
    "Example: \n",
    "-----------------------------------\n",
    "\"What is the capital of France? And who is Prince\" -> \"[Geography]What is the capital of France?,[People]Who is Prince?\"\n",
    "-----------------------------------\n",
    "Answer:\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "qa_prompt = PromptTemplate(\n",
    "    \"\"\"You are a helpful assistant that answers questions. \n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Context: \n",
    "-----------------------------------\n",
    "{context}\n",
    "-----------------------------------\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "validate_prompt = PromptTemplate(\n",
    "    \"\"\"You are a helpful AI assistant that validates, corrects and combines information in answers to a query\n",
    "Query:\n",
    "-----------------------------------\n",
    "{query_stry}\n",
    "-----------------------------------\n",
    "\n",
    "Answers:\n",
    "-----------------------------------\n",
    "{answers}\n",
    "-----------------------------------\n",
    "\n",
    "Validate, correct and combine the answers to provide a single coherent response.\n",
    "Answer:    \n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "\n",
    "class RewriteRetrieveReadQueryEngine(CustomQueryEngine):\n",
    "    \"\"\"RAG String Query Engine.\"\"\"\n",
    "\n",
    "    categories: list[str]\n",
    "    descriptions: list[str]\n",
    "    retrievers: Dict[str, BaseRetriever]\n",
    "    llm: OpenAI = OpenAI(api_key=OPENAI_API_KEY, model=\"gpt-4-turbo\")\n",
    "    verbose: bool = False\n",
    "\n",
    "    def custom_query(self, query_str: str):\n",
    "        categories_with_descriptions = \"\\n\".join(\n",
    "            [\n",
    "                f\"{category} - {description}\"\n",
    "                for category, description in zip(self.categories, self.descriptions)\n",
    "            ]\n",
    "        )\n",
    "        rewrite_prompt = rewrite_prompt_template.format(\n",
    "            query_str=query_str,\n",
    "            categories_with_descriptions=categories_with_descriptions,\n",
    "        )\n",
    "        rewrite_res = self.llm.complete(rewrite_prompt)\n",
    "            \n",
    "        questions = str(rewrite_res).replace(\"\\\"\", \"\").split(\",\")\n",
    "        if self.verbose:\n",
    "            print (\"Questions:\", questions)\n",
    "            \n",
    "        answers = []\n",
    "        for question in questions:\n",
    "            category, q = question[1:].split(\"]\")\n",
    "            if self.verbose:\n",
    "                print(\"\\n\\nRetrieving answer for question:\", q)\n",
    "                print(\"Using category:\", category)\n",
    "            nodes = self.retrievers[category].retrieve(q)\n",
    "            if self.verbose:\n",
    "                print(\"Retrieved nodes:\", nodes)\n",
    "            context = \"\\n\\n\".join([n.node.get_content() for n in nodes])\n",
    "            answer = self.llm.complete(\n",
    "                qa_prompt.format(question=q, context=context)\n",
    "            )\n",
    "            if self.verbose:\n",
    "                print(\"Answer:\", answer)\n",
    "            answers.append(answer.text)\n",
    "        if self.verbose:\n",
    "            print(\"\\n\\nValidating answers\")\n",
    "        response = self.llm.complete(\n",
    "            validate_prompt.format(query_stry=query_str, answers=\"\\n\".join(answers))\n",
    "        )\n",
    "\n",
    "        return str(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = [\"VAN_GOGH\", \"AMSTERDAM\"]\n",
    "descriptions = [\"Questions about Vincent van Gogh\", \"Questions about Amsterdam\"]\n",
    "retrievers = {\"VAN_GOGH\": vg_index.as_retriever(similarity_top_k=2), \"AMSTERDAM\": a_index.as_retriever(similarity_top_k=2)}\n",
    "\n",
    "query_engine = RewriteRetrieveReadQueryEngine(\n",
    "    categories=categories,\n",
    "    descriptions=descriptions,\n",
    "    retrievers=retrievers,\n",
    "    llm=llm,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "Hello, I am a student who is interested in learning about art and history. I have two questions that I would like to know more about.\n",
    "Who is Vincent van Gogh and what is Amsterdam famous for?\n",
    "\n",
    "Kind regards, Billy\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = query_engine.query(query)\n",
    "print(\"Response:\", response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypothetical Document Embeddings (HyDE)\n",
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step-Back Prompting\n",
    "TODO"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
