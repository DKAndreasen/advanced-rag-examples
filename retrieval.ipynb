{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieval\n",
    "\n",
    "In baseline RAG, retrieval is usually done simply by using vector search. However, as the field has developed, researchers have discovered multiple ways to enhance the retrieval process. This notebook will cover the following retrieval methods: \n",
    "\n",
    "- **Iterative Retrieval** - Use the reasoning capabilities of LLMs to perform iterative retrieval-generation cycles until passing an evaluation step.\n",
    "- **Recursive-Retrieval** - Search for smaller documents and use chunk references or metadata references to retrieve the full document.\n",
    "- **Generator-Enhanced Retrieval** - Using a LLM to predict when and what to retrieve across generation with a process called [**F**orward-**L**ooking **A**ctive **RE**trieval augmented\n",
    "generation (FLARE)](https://arxiv.org/pdf/2305.06983)\n",
    "- **GraphRAG** - Using existing or LLM-generated knowledge graphs to enhance retrieval by harnessing the relationships between entities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup libraries and environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install python-dotenv  ipython-ngql llama-index==0.10.33 llama-index-llms-openai==0.1.16 llama-index-readers-wikipedia==0.1.4 llama-index-readers-papers==0.1.4 llama-index-readers-web==0.1.12 llama-index-graph-stores-nebula==0.1.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import copy\n",
    "import json\n",
    "\n",
    "\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "from util.helpers import (\n",
    "    get_malazan_pages,\n",
    "    create_and_save_md_files,\n",
    ")\n",
    "\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding, OpenAIEmbeddingModelType\n",
    "from llama_index.core import (\n",
    "    SimpleDirectoryReader,\n",
    "    VectorStoreIndex,\n",
    "    KnowledgeGraphIndex,\n",
    "    PromptTemplate,\n",
    "    StorageContext,\n",
    "    Settings,\n",
    ")\n",
    "from llama_index.core.query_engine import (\n",
    "    FLAREInstructQueryEngine,\n",
    "    RetrieverQueryEngine,\n",
    "    RetryQueryEngine,\n",
    "    RetryGuidelineQueryEngine,\n",
    "    KnowledgeGraphQueryEngine,\n",
    ")\n",
    "from llama_index.core.prompts import PromptType\n",
    "\n",
    "from llama_index.legacy.query_engine import KnowledgeGraphQueryEngine as LegacyKnowledgeGraphQueryEngine\n",
    "\n",
    "from llama_index.core.retrievers import KnowledgeGraphRAGRetriever\n",
    "\n",
    "from llama_index.readers.web import SimpleWebPageReader\n",
    "from llama_index.readers.papers import ArxivReader\n",
    "from llama_index.readers.wikipedia import WikipediaReader\n",
    "from llama_index.graph_stores.nebula import NebulaGraphStore\n",
    "from llama_index.readers.wikipedia import WikipediaReader\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.core.schema import IndexNode\n",
    "from llama_index.core.extractors import (\n",
    "    SummaryExtractor,\n",
    "    QuestionsAnsweredExtractor,\n",
    ")\n",
    "from llama_index.core.retrievers import RecursiveRetriever\n",
    "from llama_index.core.evaluation import (\n",
    "    RelevancyEvaluator,\n",
    "    GuidelineEvaluator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: This is ONLY necessary in jupyter notebook.\n",
    "# Details: Jupyter runs an event-loop behind the scenes.\n",
    "#          This results in nested event-loops when we start an event-loop to make async queries.\n",
    "#          This is normally not allowed, we use nest_asyncio to allow it for convenience.\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "llm = OpenAI(api_key=OPENAI_API_KEY, model=\"gpt-4-turbo\")\n",
    "embed_model = OpenAIEmbedding(api_key=OPENAI_API_KEY, model=OpenAIEmbeddingModelType.TEXT_EMBED_3_SMALL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read papers from arXiv as documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = ArxivReader()\n",
    "reader.load_data\n",
    "\n",
    "papers = [\"2404.10981\", \"2305.06983\", \"2312.10997\"]\n",
    "papers_dir = \"./data/docs/arxiv\"\n",
    "\n",
    "arxiv_res = [reader.load_papers_and_abstracts(search_query=f\"id:{paper}\", max_results=1, papers_dir=papers_dir) for paper in papers]\n",
    "documents = [doc for sublist in [d for (d, _) in arxiv_res] for doc in sublist]\n",
    "abstracts = [a[0] for (_, a) in arxiv_res]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "papers_index = VectorStoreIndex.from_documents(documents=documents, show_progress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iterative Retrieval\n",
    "In **Iterative Retrieval**, the key part of the process is the evaluator or *judge* that helps self-correct the retrieval process. \n",
    "\n",
    "The pipeline performs the following steps:\n",
    "1. first queries the base query engine, then\n",
    "2. use the evaluator to decided if the response passes.\n",
    "3. If the response passes, then return response,\n",
    "4. Otherwise, transform the original query with the evaluation result (query, response, and feedback) into a new query,\n",
    "5. Repeat up to max_retries\n",
    "\n",
    "There's different types of evaluators that can be used, depending on the use-case. Sometimes it might be useful to evaluate the answer specifically, or the context. Other times you might need to create some guidelines for the LLM to use in order to evaluate whether to perform another retrieval step or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Prompt of RelevancyEvaluator\n",
    "DEFAULT_EVAL_TEMPLATE = PromptTemplate(\n",
    "    \"Your task is to evaluate if the response for the query \\\n",
    "    is in line with the context information provided.\\n\"\n",
    "    \"You have two options to answer. Either YES/ NO.\\n\"\n",
    "    \"Answer - YES, if the response for the query \\\n",
    "    is in line with context information otherwise NO.\\n\"\n",
    "    \"Query and Response: \\n {query_str}\\n\"\n",
    "    \"Context: \\n {context_str}\\n\"\n",
    "    \"Answer: \"\n",
    ")\n",
    "\n",
    "DEFAULT_REFINE_TEMPLATE = PromptTemplate(\n",
    "    \"We want to understand if the following query and response is\"\n",
    "    \"in line with the context information: \\n {query_str}\\n\"\n",
    "    \"We have provided an existing YES/NO answer: \\n {existing_answer}\\n\"\n",
    "    \"We have the opportunity to refine the existing answer \"\n",
    "    \"(only if needed) with some more context below.\\n\"\n",
    "    \"------------\\n\"\n",
    "    \"{context_msg}\\n\"\n",
    "    \"------------\\n\"\n",
    "    \"If the existing answer was already YES, still answer YES. \"\n",
    "    \"If the information is present in the new context, answer YES. \"\n",
    "    \"Otherwise answer NO.\\n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = RelevancyEvaluator(llm=llm)\n",
    "query_engine = RetryQueryEngine(query_engine=papers_index.as_query_engine(), evaluator=evaluator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = GuidelineEvaluator(llm=llm, guidelines=\"List elements of the answer as bullets\")\n",
    "query_engine = RetryGuidelineQueryEngine(query_engine=papers_index.as_query_engine(), resynthesize_query=True, guideline_evaluator=evaluator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What are the phases of Advanced RAG?\"\n",
    "response = query_engine.query(str_or_query_bundle=query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response.metadata)\n",
    "print(str(response))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recursive-Retrieval\n",
    "\n",
    "In **Recursive-Retrieval**, we search to relevant documents using references between smaller documents and their related larger documents. This enables us to optimize the narrowing the search space for the first retrieval, and then recursively increasing the context using the references.\n",
    "\n",
    "We distinguish between two types of recursive-retrieval:\n",
    "1. **Chunk-References** - References between smaller and larger documents.\n",
    "2. **Metadata-References** - References between metadata like summaries or generated questions to documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chunk References"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = WikipediaReader()\n",
    "wiki_documents = reader.load_data(pages=[\"Vincent Van Gogh\"])\n",
    "wiki_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "node_parser = SentenceSplitter(chunk_size=1024)\n",
    "base_nodes = node_parser.get_nodes_from_documents(documents=wiki_documents, show_progress=True)\n",
    "# set node ids to be a constant\n",
    "for idx, node in enumerate(base_nodes):\n",
    "    node.id_ = f\"node-{idx}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_chunk_sizes = [256, 512]\n",
    "sub_node_parsers = [\n",
    "    SentenceSplitter(chunk_size=c, chunk_overlap=20) for c in sub_chunk_sizes\n",
    "]\n",
    "\n",
    "all_nodes = []\n",
    "for base_node in base_nodes:\n",
    "    for n in sub_node_parsers:\n",
    "        sub_nodes = n.get_nodes_from_documents([base_node])\n",
    "        sub_inodes = [\n",
    "            IndexNode.from_text_node(sn, base_node.node_id) for sn in sub_nodes\n",
    "        ]\n",
    "        all_nodes.extend(sub_inodes)\n",
    "\n",
    "    # also add original node to node\n",
    "    original_node = IndexNode.from_text_node(base_node, base_node.node_id)\n",
    "    all_nodes.append(original_node)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test the effectiveness of chunk references go to \"**Test recursive-retrieval engine**\" section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metadata References"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extractors = [\n",
    "    SummaryExtractor(summaries=[\"self\"], llm=OpenAI(api_key=OPENAI_API_KEY, model=\"gpt-3.5-turbo\"), show_progress=True),\n",
    "    QuestionsAnsweredExtractor(questions=5, llm=OpenAI(api_key=OPENAI_API_KEY, model=\"gpt-3.5-turbo\"), show_progress=True),\n",
    "]\n",
    "# run metadata extractor across base nodes, get back dictionaries\n",
    "node_to_metadata = {}\n",
    "for extractor in extractors:\n",
    "    metadata_dicts = extractor.extract(base_nodes)\n",
    "    for node, metadata in zip(base_nodes, metadata_dicts):\n",
    "        if node.node_id not in node_to_metadata:\n",
    "            node_to_metadata[node.node_id] = metadata\n",
    "        else:\n",
    "            node_to_metadata[node.node_id].update(metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_metadata_dicts(path, data):\n",
    "    with open(path, \"w\") as fp:\n",
    "        json.dump(data, fp)\n",
    "\n",
    "\n",
    "def load_metadata_dicts(path):\n",
    "    with open(path, \"r\") as fp:\n",
    "        data = json.load(fp)\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "path = \"./data/retrieval\"\n",
    "if not os.path.exists(path):\n",
    "    print(\"Creating directory: \", path)\n",
    "    os.makedirs(path)\n",
    "\n",
    "save_metadata_dicts(path + \"/metadata_dicts.json\", node_to_metadata)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_dicts = load_metadata_dicts(\"./data/retrieval/metadata_dicts.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_nodes = copy.deepcopy(base_nodes)\n",
    "for node_id, metadata in node_to_metadata.items():\n",
    "    print(node_id)\n",
    "    for val in metadata.values():\n",
    "        all_nodes.append(IndexNode(text=val, index_id=node_id))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test recursive-retrieval engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "all_nodes_dict = {n.node_id: n for n in all_nodes}\n",
    "recursive_index = VectorStoreIndex(all_nodes, embed_model=embed_model, show_progress=True)\n",
    "retriever = RecursiveRetriever(\n",
    "    \"vector\",\n",
    "    retriever_dict={\"vector\": recursive_index.as_retriever(similarity_top_k=2)},\n",
    "    node_dict=all_nodes_dict,\n",
    "    verbose=True,\n",
    ")\n",
    "recursive_query_engine = RetrieverQueryEngine.from_args(retriever=retriever, llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = recursive_query_engine.query(\"What are the phases of Advanced RAG?\")\n",
    "print(response.metadata)\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generator-Enhanced Retrieval\n",
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GraphRAG\n",
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you don't already have NebulaGraph running locally, you can use [nebula-up](https://github.com/wey-gu/nebula-up) to start it up.\n",
    "\n",
    "Run the following command in your terminal (On Windows you should use WSL):\n",
    "\n",
    "```bash\n",
    "curl -fsSL nebula-up.siwei.io/install.sh | bash\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your NebulaGraph is started with nebula-up, you can add the following to your .env file:\n",
    "\n",
    "```\n",
    "NEBULA_USER=root\n",
    "NEBULA_PASSWORD=nebula\n",
    "NEBULA_HOST=localhost\n",
    "NEBULA_PORT=9669\n",
    "```\n",
    "\n",
    "Or set with your own configurations for NebulaGraph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext ngql\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(override=True)\n",
    "GRAPHD_HOST = os.getenv(\"NEBULA_HOST\")\n",
    "GRAPHD_PORT = os.getenv(\"NEBULA_PORT\")\n",
    "NEBULA_PASSWORD = os.getenv(\"NEBULA_PASSWORD\")\n",
    "NEBULA_USER = os.getenv(\"NEBULA_USER\")\n",
    "NEBULA_ADDRESS = f\"{GRAPHD_HOST}:{GRAPHD_PORT}\"\n",
    "os.environ[\"NEBULA_ADDRESS\"] = NEBULA_ADDRESS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "connection_string = f\"--address {GRAPHD_HOST} --port {GRAPHD_PORT} --user {NEBULA_USER} --password {NEBULA_PASSWORD}\"\n",
    "%ngql {connection_string}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create new knowledge graph (**Graph Space** in Nebula terms) to use for retrieval. We'll create an entity-relationship graph with the following schema:\n",
    "```\n",
    "[entity:tag] - [relationship:edge] -> [entity:tag]\n",
    "```\n",
    "\n",
    "This will allow the LLM to automatically generate our knowledge graph with arbitrary relationships like:\n",
    "```\n",
    "[entity:David] - [relationship:has studied computer science at] -> [entity:Aarhus University]\n",
    "[entity:David] - [relationship:currently works as a software pilot at] -> [entity:Trifork A/S]\n",
    "```\n",
    "\n",
    "We also generate an index for the name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%ngql CREATE SPACE IF NOT EXISTS graph_rag(vid_type=FIXED_STRING(256), partition_num=1, replica_factor=1);\n",
    "%ngql SHOW SPACES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%ngql\n",
    "USE graph_rag;\n",
    "CREATE TAG IF NOT EXISTS entity(name string);\n",
    "CREATE EDGE IF NOT EXISTS relationship(relationship string);\n",
    "CREATE TAG INDEX IF NOT EXISTS entity_index ON entity(name(256));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%ngql USE rag_workshop; CLEAR SPACE rag_workshop; # clean graph space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load documents we want to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pages = get_malazan_pages(articles=[\"Anomander Rake\"])\n",
    "docs_path = \"./data/docs/graph_rag\"\n",
    "create_and_save_md_files(pages, path=docs_path + \"/\")\n",
    "\n",
    "documents = SimpleDirectoryReader(input_dir=docs_path).load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use LlamaIndex's `KnowledgeGraphIndex` to index the graph and use it for retrieval. It uses the following default prompt to generate triplets for the graph:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   ```\n",
    "    Some text is provided below. Given the text, extract up to \n",
    "    {max_knowledge_triplets} \n",
    "    knowledge triplets in the form of (subject, predicate, object). Avoid stopwords.\n",
    "    ---------------------\n",
    "    Example:\n",
    "    Text: Alice is Bob's mother.\n",
    "    Triplets:\n",
    "    (Alice, is mother of, Bob)\n",
    "    Text: Philz is a coffee shop founded in Berkeley in 1982.\n",
    "    Triplets:\n",
    "    (Philz, is, coffee shop)\n",
    "    (Philz, founded in, Berkeley)\n",
    "    (Philz, founded in, 1982)\n",
    "    ---------------------\n",
    "    Text: {text}\n",
    "    Triplets:\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"NEBULA_ADDRESS\"] = NEBULA_ADDRESS\n",
    "os.environ[\"NEBULA_PASSWORD\"] = NEBULA_PASSWORD\n",
    "os.environ[\"NEBULA_USER\"] = NEBULA_USER\n",
    "\n",
    "space_name = \"graph_rag\"\n",
    "edge_types, rel_prop_names = [\"relationship\"], [\"relationship\"]\n",
    "tags = [\"entity\"]\n",
    "\n",
    "graph_store = NebulaGraphStore(\n",
    "    space_name=space_name,\n",
    "    edge_types=edge_types,\n",
    "    rel_prop_names=rel_prop_names,\n",
    "    tags=tags,\n",
    ")\n",
    "\n",
    "Settings.llm = OpenAI(api_key=OPENAI_API_KEY, model=\"gpt-4-turbo\")\n",
    "Settings.embed_model = OpenAIEmbedding(api_key=OPENAI_API_KEY, model=OpenAIEmbeddingModelType.TEXT_EMBED_3_SMALL)\n",
    "Settings.chunk_size = 512\n",
    "storage_context = StorageContext.from_defaults(graph_store=graph_store)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This may take a little while. Afterwards we save the index to disk so we don't have to do it again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kg_index = KnowledgeGraphIndex.from_documents(\n",
    "    documents,\n",
    "    storage_context=storage_context,\n",
    "    max_triplets_per_chunk=10,\n",
    "    space_name=space_name,\n",
    "    edge_types=edge_types,\n",
    "    rel_prop_names=rel_prop_names,\n",
    "    tags=tags,\n",
    "    show_progress=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#kg_index.storage_context.persist(persist_dir='./data/storage_graph')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import load_index_from_storage\n",
    "\n",
    "storage_context = StorageContext.from_defaults(persist_dir='./data/storage_graph', graph_store=graph_store)\n",
    "kg_index = load_index_from_storage(\n",
    "    storage_context=storage_context,\n",
    "    max_triplets_per_chunk=10,\n",
    "    space_name=space_name,\n",
    "    edge_types=edge_types,\n",
    "    rel_prop_names=rel_prop_names,\n",
    "    tags=tags,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kg_index_query_engine = kg_index.as_query_engine(\n",
    "    retriever_mode=\"keyword\",\n",
    "    verbose=True,\n",
    "    response_mode=\"tree_summarize\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%ngql MATCH p=(n)-[e:relationship*1..2]-(m) WHERE id(n) in ['Anomander'] RETURN n.entity.name,e[0].relationship, m.entity.name LIMIT 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%ngql MATCH p=(n)-[e:relationship*1..2]-(m) WHERE id(n) in ['Rake'] RETURN n.entity.name,e[0].relationship, m.entity.name LIMIT 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kg_index_query_engine.query(\"Tell me about Anomander Rakes relationship with the Malazan Empire\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NL2Cypher\n",
    "\n",
    "It's also possible that we might already have existing knowledge graphs with useful information which hasn't been indexed with embeddings. This would prohibit us from using the vector index to find the right entities/relationships. In this case its possible to use LLMs to automatically generate Cypher queries to retrieve information from the graph based on the query.\n",
    "\n",
    "NL2Cypher is a tool that can be used to generate Cypher queries from natural language queries. We can use it to generate Cypher queries for our knowledge graph.\n",
    "\n",
    "Currently this tool doesn't work super well with the Knowledge Space generated about \"Anomander Rake\", this shows how you might use it with a different knowledge graph.\n",
    "Some fine-tuning might be needed to get the better results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_NEBULAGRAPH_NL2CYPHER_PROMPT_TMPL = \"\"\"\n",
    "Generate NebulaGraph query from natural language.\n",
    "Use only the provided relationship types and properties in the schema.\n",
    "Do not use any other relationship types or properties that are not provided.\n",
    "Schema:\n",
    "---\n",
    "{schema}\n",
    "---\n",
    "Note: NebulaGraph speaks a dialect of Cypher, comparing to standard Cypher:\n",
    "\n",
    "1. it uses double equals sign for comparison: `==` rather than `=`\n",
    "2. it needs explicit label specification when referring to node properties, i.e.\n",
    "v is a variable of a node, and we know its label is Foo, v.`foo`.name is correct\n",
    "while v.name is not.\n",
    "\n",
    "For example, see this diff between standard and NebulaGraph Cypher dialect:\n",
    "```diff\n",
    "< MATCH (p:person)-[:directed]->(m:movie) WHERE m.name = 'The Godfather'\n",
    "< RETURN p.name;\n",
    "---\n",
    "> MATCH (p:`person`)-[:directed]->(m:`movie`) WHERE m.`movie`.`name` == 'The Godfather'\n",
    "> RETURN p.`person`.`name`;\n",
    "```\n",
    "\n",
    "Question: {query_str}\n",
    "\n",
    "NebulaGraph Cypher dialect query:\n",
    "\"\"\"\n",
    "DEFAULT_NEBULAGRAPH_NL2CYPHER_PROMPT = PromptTemplate(\n",
    "    DEFAULT_NEBULAGRAPH_NL2CYPHER_PROMPT_TMPL,\n",
    "    prompt_type=PromptType.TEXT_TO_GRAPH_QUERY,\n",
    ")\n",
    "\n",
    "query_engine_with_nl2graphquery =  KnowledgeGraphQueryEngine(\n",
    "    storage_context=storage_context,\n",
    "    llm=OpenAI(api_key=OPENAI_API_KEY),\n",
    "    verbose=True,\n",
    "    graph_query_synthesis_prompt=DEFAULT_NEBULAGRAPH_NL2CYPHER_PROMPT\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine_with_nl2graphquery.generate_query(\"Tell me Anomander's relationship with Andarist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = query_engine_with_nl2graphquery.query(\n",
    "    \"Tell me about Anomander's relationship Andarist\",\n",
    ")\n",
    "display(Markdown(f\"<b>{response}</b>\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
